---
title: "Can Symptoms Surveys Improve COVID-19 Forecasts?"
author: "Ryan Tibshirani"
date: 2020-09-19
tags: ["symptom surveys"]
output:
  html_document:
    code_folding: hide
---

Following up on our last two posts, which covered our COVID-19 symptom surveys 
through 
[Facebook](https://delphi.cmu.edu/blog/2020/08/26/covid-19-symptom-surveys-through-facebook/)
and 
[Google](https://delphi.cmu.edu/blog/2020/09/18/covid-19-symptom-surveys-through-google/),
we take a deeper technical dive into empirical analysis, and examine whether the
% CLI-in-community indicators from our two surveys can be used to improve the 
accuracy of short-term forecasts of county-level COVID-19 case rates.

<!--more-->

Following up on our last two posts, which covered our COVID-19 symptom surveys 
through 
[Facebook](https://delphi.cmu.edu/blog/2020/08/26/covid-19-symptom-surveys-through-facebook/)
and 
[Google](https://delphi.cmu.edu/blog/2020/09/18/covid-19-symptom-surveys-through-google/),
we take a deeper technical dive into empirical analysis, and examine whether the
% CLI-in-community indicators from our two surveys can be used to improve the 
accuracy of short-term forecasts of county-level COVID-19 case rates.

**Forecasting** is one of Delphi's main initiatives 
(in the past for flu, and currently for COVID-19), 
and each week since mid-July we've been submitting forecasts 
to the [COVID Forecast Hub](https://covid19forecasthub.org), 
which serves as the official data source for the 
[CDC's communications on COVID-19 forecasts](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/mathematical-modeling.html).
To be clear, what follows is meant as a *demo*, 
and not an authoritative report on cutting-edge forecasting. 
The purpose is to study whether the Facebook and Google % CLI-in-community 
signals provide value, when used as features, on top of what we can get 
from fairly simple time series models that use case rates alone.
We'll follow up in a future blog post with details on our forecasters 
"in production". 

We're also working on a complementary problem to forecasting,
which may be dubbed **hotspot detection**: the goal here is to predict 
whether case rates will rise significantly in the coming weeks, rather 
than predict the future case rates directly, as in forecasting. 
Some of our [previous exploratory 
investigations](https://delphi.cmu.edu/blog/2020/08/26/covid-19-symptom-surveys-through-facebook/#some-interesting-examples) 
suggested that our survey signals could serve as **early indicators**
of COVID-19 activity, and hence could help with hotspot detection. Stay
tuned into the Delphi blog for a post on hotspot detection soon.

Next we explain the forecasting setup and results in detail; some parts may get 
a bit technical, but you should be able to glean the main results even if you 
skip over the details.

## Problem Setup 

We consider predicting county-level COVID-19 case incidence rates, 1 and 2 
weeks ahead, that is, we consider predicting the number of new COVID-19 cases
per capita, over the next 1-7 days and over the next 8-14 days. 
An equivalent way to phrase this problem 
(which happens to be more convenient for us) 
is to predict the *smoothed* COVID-19 case incidence rate 7 days ahead and 
14 days ahead, where the smoothing is done via a 7 day trailing average.
We restrict our attention to the 440 counties that had at least 200 confirmed
cases by May 14, 2020 (the end of the Google survey data) and in which both 
the Facebook and Google % CLI-in-community signals are available (there were 
604 counties in total with at least 200 confirmed cases by May 14, and we 
dropped 164 of them due to a lack of Facebook or Google survey data).

To fix notation, let $Y_{\ell,t}$ denote the smoothed COVID-19 case incidence 
rate for location (county) $\ell$ and time (day) $t$. Let $F_{\ell,t}$ and 
$G_{\ell,t}$ denote the Facebook and Google % CLI-in-community signals, 
respectively, for location $\ell$ and time $t$. (We rescale all these signals 
from their given values in our API so that they are true proportions: between 0 
and 1.) We evaluate the following four models:
$$
\begin{aligned}
&\text{Cases:} \quad && h(Y_{\ell,t+d})
\approx \alpha + \sum_{j=0}^2 \beta_j h(Y_{\ell,t-7j}) \\
&\text{Cases + Facebook:} \quad && h(Y_{\ell,t+d})
\approx \alpha + \sum_{j=0}^2 \beta_j h(Y_{\ell,t-7j}) +
\sum_{j=0}^2 \gamma_j h(F_{\ell,t-7j}) \\
&\text{Cases + Google:} \quad && h(Y_{\ell,t+d})
\approx \alpha + \sum_{j=0}^2 \beta_j h(Y_{\ell,t-7j}) +
\sum_{j=0}^2 \gamma_j h(G_{\ell,t-7j}) \\
&\text{Cases + Facebook + Google:} \quad && h(Y_{\ell,t+d})
\approx \alpha + \sum_{j=0}^2 \beta_j h(Y_{\ell,t-7j}) +
\sum_{j=0}^2 \gamma_j h(F_{\ell,t-7j}) +
\sum_{j=0}^2 \tau_j h(G_{\ell,t-7j}).
\end{aligned}
$$
Here $d=7$ or $d=14$, depending on the target value (number of days we predict
ahead), and $h$ is a transformation to be specified later. In words, in the 
first model we're using current COVID-19 case rates, as well as those 1 and 2 
weeks back, in order to predict future case rates; in the second, we're 
additionally using the current Facebook signal, and this signal 1 and 2 weeks 
back; in the third, it's the same but with the Google signal instead of the
Facebook one; and in the fourth, we use both Facebook and Google signals. For 
each model, in order to make a forecast at time $t_0$ (to predict case rates 
for time $t_0+d$), we fit the model using least absolute deviations (LAD) 
regression, training over all $\ell$ (all 440 counties), and all $t$ that are 
within the most recent 14 days of data available up to and including time $t_0$. 

Forecasts are transformed back to the original scale (we apply $h^{-1}$ to the
predictions from the fitted LAD model), and denoted $\hat{Y}_{\ell,t_0+d}$. For 
an error metric, we consider **scaled absolute error** (or just scaled error for 
short):
$$
\frac{|\hat{Y}_{\ell,t_0+d} - Y_{\ell,t_0+d}|}
{|Y_{\ell,t_0} - Y_{\ell,t_0+d}|},
$$
where the error in the denominator is the error of the "strawman" model, which
for any target always simply predicts the most recent available case rate. 

This normalization helps for two reasons. First, it gives us an interpretable
scale, as we can understand the scaled error as a fraction improvement over 
the strawman's error (so numbers like 0.8 or 0.9 would be favorable, and 
numbers like 2 or 5 or 10 would be increasingly disastrous). Second, in our
forecasting problem there turns out to a considerable amount of county-to-county
of variability in forecasting difficulty, and normalizing by the strawman's
error helps adjust for this (so that the aggregate results aren't dominated
by county-to-county differences).

## Transformations

We investigated three transformations $h$: identity, log, and logit (the 
latter two being common variance-stabilizing transforms for proportions). 
The results in all three cases were quite similar 
and the qualitative conclusions don't change at all 
(the code below supports all three, so you can check this for yourself). 
For brevity, we'll just show the results for the logit transform 
(actually, a "padded" version $h(x) = \log\left(\frac{x+a}{1-x+a}\right)$, where
the numerator and denominator are pushed away from zero by a small constant, 
which we took to be $a=0.01$). 

## Forecasting Code

The code below marches the forecast date $t_0$ forward, one day at a time (from 
April 11 to September 1), fits the four models, makes 7 and 14 day ahead 
predictions, and records errors. It takes a little while to run, the culprit 
being LAD regression: here the training sets get moderately large (aggregating 
the data over 440 counties and 14 days results in over 6000 training samples), 
and at this scale LAD regression is much slower than (say) least squares
regression. We ran this R code separately and saved the results in an RData 
file; you can find this in the [same GitHub repo as that containing the Rmd 
source for this blog
post](https://github.com/cmu-delphi/delphi-blog/tree/main/content/post).

```{r, eval = FALSE, code = readLines("forecast-demo/demo.R")}
```

## Results: All Four Models

We first compare the results across all four models. For this analysis, we 
filter down to common forecast dates available for the four models (to set an
even footing for the comparison), which ends up being May 6 through May 14 for 
7 day ahead forecasts, and only May 13 through May 14 for 14 day ahead 
forecasts. (The reason for this shortened period: we paused running the Google
survey on May 14 so its data ends there, but as we explained in our last post,
we [plan to bring it 
back](https://delphi.cmu.edu/blog/2020/09/18/covid-19-symptom-surveys-through-google/#google-survey-redux)
later this fall.) Hence we skip studying the 14 day ahead forecasts results in
this four-way model discussion, as they're only based on 2 days of test data.

Below we compute and print the median scaled errors for each of the four models 
over the 9 day test period (recall that the scaled error is the absolute error 
of the model's forecast relative to that of the strawman; and each test day 
actually comprises 440 forecasts, over the 440 counties being considered). We 
can see that adding either or both of the survey signals improves on the median 
scaled error of the model that uses cases only, with the biggest gain achieved 
by the "Cases + Google" model. We can also see that the median scaled errors are
all close to 1 (with all but that from "Cases + Google" model exceeding 1),
which speaks to the difficulty of the forecasting problem.

```{r, message = FALSE, warning = FALSE}
load("forecast-demo/demo.rda")
library(dplyr)
library(tidyr)
library(ggplot2)

# Restrict to common period for all 4 models, then calculate the scaled errors 
# for each model, that is, the error relative to the strawman's error
res_all4 = res %>%
  drop_na() %>%                                       # Restrict to common time
  mutate(err1 = err1 / err0, err2 = err2 / err0,      # Compute relative error
         err3 = err3 / err0, err4 = err4 / err0) %>%  # to strawman model
  mutate(dif12 = err1 - err2, dif13 = err1 - err3,    # Compute differences
         dif14 = err1 - err4) %>%                     # relative to cases model
  ungroup() %>%
  select(-err0) 
         
# Calculate and print median errors, for all 4 models, and just 7 days ahead
res_err4 = res_all4 %>% 
  select(-starts_with("dif")) %>%
  pivot_longer(names_to = "model", values_to = "err",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, 
                        labels = c("Cases", "Cases + Facebook", 
                                   "Cases + Google",
                                   "Cases + Facebook + Google")))

knitr::kable(res_err4 %>% 
               group_by(model, lead) %>%
               summarize(err = median(err), n = length(unique(time_value))) %>% 
               arrange(lead) %>% ungroup() %>%
               rename("Model" = model, "Median scaled error" = err, 
                      "Target" = lead, "Test days" = n) %>%
               filter(Target == "7 days ahead"), 
             caption = paste("Test period:", min(res_err4$time_value), "to",
                             max(res_err4$time_value)),
             format = "html", table.attr = "style='width:70%;'")
```

Are these differences in median scaled errors significant? It's hard to say, but
some basic hypothesis testing suggests that they probably are: below we conduct 
a [sign test](https://en.wikipedia.org/wiki/Sign_test)[^1] for whether the 
difference in the "Cases" model's scaled error and each other model's scaled 
error is centered at zero. The sign test is run on the 9 test days x 440 
counties = 3960 pairs of scaled errors. The p-values from the "Cases" versus 
"Cases + Facecbook" and the "Cases" versus "Cases + Google" tests are tiny; the
p-value from the "Cases" versus "Cases + Facebook + Google" test is much bigger
but still below 0.01. 

[^1]: As far as nonparametric tests of medians go, [Wilcoxon's signed-rank 
test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) (for paired data, 
as we have here) is more popular, because it tends to be more powerful than the 
sign test. Applied here, it does indeed give smaller p-values pretty much across
the board. However, it assumes symmetry of the distribution in question (in our 
case, the difference in scaled errors), whereas the sign test does not, and thus
we report results from the latter.

```{r, message = FALSE, warning = FALSE}
# Compute p-values using the sign test against a one-sided alternative, for
# all models, and just 7 days ahead
res_dif4 = res_all4 %>%
  select(-starts_with("err")) %>%
  pivot_longer(names_to = "model", values_to = "dif",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, 
                        labels = c("Cases vs Cases + Facebook",
                                   "Cases vs Cases + Google",
                                   "Cases vs Cases + Facebook + Google"))) 

knitr::kable(res_dif4 %>%
               group_by(model, lead) %>%
               summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE), 
                                        n = n(), alt = "greater")$p.val) %>%
               ungroup() %>% filter(lead == "7 days ahead") %>%
               rename("Comparison" = model, "Target" = lead, "P-value" = p), 
             format = "html", table.attr = "style='width:50%;'")
```

However, we should read these with a grain of salt: the sign test here assumes
independence of observations, which clearly can't be true, given the 
spatiotemporal structure of our forecasting problem. To mitigate the dependence 
across time (which intuitively seems to matter more than that across space), we 
recomputed these tests in a stratified way, where for each day we run a sign 
test on the scaled errors between two models over all 440 counties. The results 
are plotted as histograms below; the "Cases + Facebook" and "Cases + Google" 
models appear to deliver some decently small p-values, but the story is not as
clear with the "Cases + Facebook + Google" model. 

```{r, message = FALSE, warning = FALSE, fig.width = 9, fig.height = 3.75}
# Red, blue (similar to ggplot defaults), then yellow
ggplot_colors = c("#FC4E07", "#00AFBB", "#E7B800")

ggplot(res_dif4 %>% 
         group_by(model, lead, time_value) %>%
         summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE), 
                                  n = n(), alt = "greater")$p.val) %>%
         ungroup() %>% filter(lead == "7 days ahead"), aes(p)) +
  geom_histogram(aes(color = model, fill = model), alpha = 0.4) + 
  scale_color_manual(values = ggplot_colors) +
  scale_fill_manual(values = ggplot_colors) +
  facet_wrap(vars(lead, model)) + 
  labs(x = "P-value", y = "Count") +
  theme_bw() + theme(legend.pos = "none")
```

## Results: First Two Models

Next we focus on comparing results between the "Cases" and "Cases + Facebook" 
models only. Restricting to a common available forecast dates yields a much 
longer test period, May 6 through August 25 for 7 day ahead forecasts, and May
13 through August 18 for 14 day ahead forecasts. The median scaled errors over
the test period are computed and reported below. Now we see a decent improvement
in median scaled error for the "Cases + Facebook" model, and this is true for
both 7 and 14 day ahead forecasts.

```{r, message = FALSE, warning = FALSE}
# Restrict to common period for just models 1 and 2, then calculate the scaled 
# errors, that is, the error relative to the strawman's error
res_all2 = res %>%
  select(-c(err3, err4)) %>%
  drop_na() %>%                                       # Restrict to common time
  mutate(err1 = err1 / err0, err2 = err2 / err0) %>%  # Compute relative error
                                                      # to strawman model
  mutate(dif12 = err1 - err2) %>%                     # Compute differences
                                                      # relative to cases model
  ungroup() %>%
  select(-err0) 
         
# Calculate and print median errors, for just models 1 and 2, and both 7 and 14 
# days ahead
res_err2 = res_all2 %>% 
  select(-starts_with("dif")) %>%
  pivot_longer(names_to = "model", values_to = "err",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, labels = c("Cases", "Cases + Facebook")))
  
knitr::kable(res_err2 %>% 
               select(-starts_with("dif")) %>%
               group_by(model, lead) %>%
               summarize(err = median(err), n = length(unique(time_value))) %>% 
               arrange(lead) %>% ungroup() %>%
               rename("Model" = model, "Median scaled error" = err, 
                      "Target" = lead, "Test days" = n),
             caption = paste("Test period:", min(res_err2$time_value), "to",
                             max(res_err2$time_value)),
             format = "html", table.attr = "style='width:70%;'")
```

Thanks to the extended length of the test period, we can also "unravel" these
median scaled errors over time and plot their trajectories, as we do below,
with the left plot concerning 7 day ahead forecasts, and the right 14 day ahead 
forecasts. These plots reveal something quite interesting (and bothersome): the
median scaled errors are quite volatile over time, and for some periods in July,
forecasting became much "harder", with the scaled errors reaching above 1.25 for
7 day ahead forecasts, and above 1.5 for 14 day ahead forecasts. Furthermore, 
towards the positive, we can see a clear visual difference between the median 
scaled errors from the "Cases + Facebook" model in red and the "Cases" model in
black. The former appears to be below the latter pretty consistently over time,
with the possible exception of periods where forecasting becomes "hard" and the
scaled errors shoot above 1. 

```{r, message = FALSE, warning = FALSE, fig.width = 9, fig.height = 5}
# Plot median errors as a function of time, for models 1 and 2, and both 7 and
# 14 days ahead
ggplot(res_err2 %>% 
         group_by(model, lead, time_value) %>%
         summarize(err = median(err)),
       aes(x = time_value, y = err)) + 
  geom_line(aes(color = model)) + 
  scale_color_manual(values = c("black", ggplot_colors)) +
  geom_hline(yintercept = 1, linetype = 2, color = "gray") +
  facet_wrap(vars(lead)) + 
  labs(x = "Date", y = "Median scaled error") +
  theme_bw() + theme(legend.pos = "bottom", legend.title = element_blank())
```

Again, basic hypothesis testing suggests that the results we're seeing here are
likely significant, though it's hard to say definitively given the complicated 
dependence structure present in the data. Below we perform a sign test for
whether the difference in scaled errors from the "Cases" and "Cases + Facebook" 
models is centered at zero. Given the large sample size: 112 test days for 7 day
ahead forecasts and 98 test days for 14 day ahead forecasts (times 440 counties
for each day), the p-values are basically zero. 

```{r, message = FALSE, warning = FALSE}
# Compute p-values using the sign test against a one-sided alternative, just 
# for models 1 and 2, and both 7 and 14 days ahead
res_dif2 = res_all2 %>%
  select(-starts_with("err")) %>%
  pivot_longer(names_to = "model", values_to = "dif",
               cols = -c(geo_value, time_value, lead)) %>%
  mutate(lead = factor(lead, labels = paste(leads, "days ahead")),
         model = factor(model, labels = "Cases > Cases + Facebook"))

knitr::kable(res_dif2 %>% 
               group_by(model, lead) %>%
               summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE),
                                        n = n(), alt = "greater")$p.val) %>% 
               ungroup() %>% 
               rename("Comparison" = model, "Target" = lead, "P-value" = p), 
             format = "html", table.attr = "style='width:50%;'")
```

Once we stratify and recompute p-values by forecast date, as shown in the
histograms below, the bulk of p-values are still quite small.

```{r, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4}
ggplot(res_dif2 %>% 
         group_by(model, lead, time_value) %>%
         summarize(p = binom.test(x = sum(dif > 0, na.rm = TRUE), 
                                  n = n(), alt = "greater")$p.val) %>%
         ungroup(), aes(p)) +
  geom_histogram(aes(color = model, fill = model), alpha = 0.4) + 
  scale_color_manual(values = ggplot_colors) +
  scale_fill_manual(values = ggplot_colors) +
  facet_wrap(vars(lead, model)) + 
  labs(x = "P-value", y = "Count") +
  theme_bw() + theme(legend.pos = "none")
```

## Wrap-Up

We've seen that either of the Facebook or Google % CLI-in-community signals 
can improve the accuracy of short-term forecasts of county-level COVID-19 case
incidence rates. The statistical significance associated with these improvements 
is more apparent with the Facebook signal, thanks to the much longer test period 
available. With either signal, of the magnitude of the improvement offered seems 
modest but nontrivial, especially because the forecasting problem is so hard in 
the first place.

We reiterate that this was just a demo; our analysis was pretty simple and lacks
a few qualities that a truly comprehensive, realistic forecasting analysis 
should possess. To name three: 

1. The models we considered are simple autoregressive structures from standard 
time series, and could be improved in various ways (including, considering other
relevant dimensions like mobility measures, county health metrics, etc.).

2. The forecasts we produced are *point* rather than *distributional* forecasts
(that is, we predict a single number, rather than an entire distribution, for
what happens 7 and 14 days ahead). Distributional forecasts portray uncertainty 
in a transparent way, which is hugely important in practice.

3. The way we trained our forecasts does not account for *data latency* and 
*revisions*, which are critical issues. For each (retrospective) forecast date 
$t_0$, we constructed forecasts by training on data that we fetched from the API 
today, "as of" the day we wrote this blog post, and not "as of" the forecast 
date $t_0$. This matters because nearly all signals are subject to latency (they
are only available at some number of days lag) and go through multiple revisions 
(past data values get updated as time goes on).

On the flip side, our example here was not that "far away" from being realistic.
The models we examined are actually not too different from Delphi's forecasters
"in production".[^2] Also, the way we fit LAD regression models in the code 
extends immediately to multiple quantile regression (just requires changing the 
parameter `tau` in the call to `quantile_lasso()`), which would give us 
distributional forecasts. And lastly, it's fairly easy to change the data 
acquisition step in the code so that data gets pulled "as of" the forecast date
(requires specifying the parameter `as_of` in the call to `covidcast_signal()`).

[^2]: Delphi's forecasters "in production" are still based on relatively simple
times series models, though to be clear they're distributional, 
and we add a couple of extra layers of complexity on top of standard structures. 
For short-term forecasts, we've found that simple statistical models can be 
competitive with compartmental models (like SIR and its many variants), even 
fairly complicated ones. Being statisticians and computer scientists, we find
these statistical models are easier to build, debug, and most importantly, 
calibrate. More on this in a future blog post. 

**Acknowledgements.** *Delphi's forecasting effort involves many people from our
modeling team, from forecaster design, to implementation, to evaluation. The 
broader insights on forecasting shared in this post are not certainly 
attributable to Ryan's work alone, but are a reflection of the work carried out 
by all these team members.*

---

*[Ryan Tibshirani](http://stat.cmu.edu/~ryantibs) is a lead researcher in the
Delphi group, and an Associate Professor in the Department of Statistics & Data
Science and the Machine Learning Department at CMU. He is also an Amazon
Scholar.*
